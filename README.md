# LiveLikeALocal: Interdisciplinary Collaboration Assistant

## ğŸŒŸ Project Overview

**LiveLikeALocal** is an innovative AI-powered chatbot system designed to facilitate seamless collaboration between Computer Science and Biology teams working on DNA-related projects. This interdisciplinary assistant bridges knowledge gaps, translates domain-specific terminology, and helps team members understand concepts outside their expertise through intelligent conversation and context-aware responses.

### ğŸ¯ Primary Mission

The system serves as a sophisticated bridge between two complex domains:
- **Computer Science**: Algorithms, data structures, machine learning, software engineering
- **Biology**: DNA structures, genetic processes, molecular biology, genomic data

By providing real-time translation and contextual understanding, LiveLikeALocal enables more effective interdisciplinary collaboration, reducing communication barriers and accelerating scientific discovery.

## ğŸ—ï¸ Architecture Overview

The project follows a modern, scalable architecture with three main components:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend      â”‚    â”‚    Backend      â”‚    â”‚  Data Pipeline  â”‚
â”‚   (React.js)    â”‚â—„â”€â”€â–ºâ”‚   (Node.js)     â”‚â—„â”€â”€â–ºâ”‚   (Python)      â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ Chat UI       â”‚    â”‚ â€¢ Express API   â”‚    â”‚ â€¢ Web Scraping  â”‚
â”‚ â€¢ Real-time     â”‚    â”‚ â€¢ Ollama LLM    â”‚    â”‚ â€¢ Data Cleaning â”‚
â”‚   Messaging     â”‚    â”‚ â€¢ Pinecone DB   â”‚    â”‚ â€¢ Vector Store  â”‚
â”‚ â€¢ Chat History  â”‚    â”‚ â€¢ Embeddings    â”‚    â”‚   Population    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ”§ Technology Stack

#### Frontend
- **React.js 19.0.0** - Modern UI framework
- **CSS3** - Styling and responsive design
- **Web Vitals** - Performance monitoring

#### Backend
- **Node.js** - Server runtime
- **Express.js** - Web framework
- **Ollama** - Local LLM integration
- **Pinecone** - Vector database
- **Transformers.js** - Embedding generation
- **CORS** - Cross-origin resource sharing

#### Data Processing
- **Python** - Data scraping and processing
- **BeautifulSoup** - Web scraping
- **LangChain** - LLM orchestration
- **Sentence Transformers** - Text embeddings

## ğŸš€ Key Features

### ğŸ¤– Intelligent Conversation
- **Context-Aware Responses**: Leverages retrieved documents and chat history
- **Domain Translation**: Converts technical concepts between CS and Biology
- **Multi-turn Conversations**: Maintains conversation context across sessions
- **Real-time Processing**: Instant responses with typing indicators

### ğŸ” Advanced Search & Retrieval
- **Semantic Search**: Uses embeddings for contextually relevant document retrieval
- **Vector Database**: Pinecone integration for efficient similarity search
- **Document Context**: Provides relevant background information for responses

### ğŸ’¾ Persistent Chat Management
- **Chat Sessions**: Multiple concurrent chat sessions
- **History Persistence**: Maintains conversation history across sessions
- **Session Management**: Start, load, and clear chat sessions

### ğŸ¨ Modern User Interface
- **Responsive Design**: Works across desktop and mobile devices
- **Real-time Updates**: Live message streaming and status updates
- **Intuitive Navigation**: Sidebar chat management and easy switching
- **Professional Styling**: Clean, modern interface design

## ğŸ“ Project Structure

```
LiveLikeALocal/
â”œâ”€â”€ Backend/                    # Node.js server application
â”‚   â”œâ”€â”€ index.js               # Main server entry point
â”‚   â”œâ”€â”€ ollamaHandler.js       # LLM interaction logic
â”‚   â”œâ”€â”€ pinecone.js            # Vector database configuration
â”‚   â”œâ”€â”€ package.json           # Node.js dependencies
â”‚   â”œâ”€â”€ Initialprompt.md       # System prompt configuration
â”‚   â”œâ”€â”€ graduate seminar (2).ipynb  # Development notebook
â”‚   â””â”€â”€ test-*.js              # Test files for various components
â”‚
â”œâ”€â”€ Frontend/                   # React.js client application
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.js             # Main React component
â”‚   â”‚   â”œâ”€â”€ App.css            # Application styling
â”‚   â”‚   â””â”€â”€ components/        # Reusable UI components
â”‚   â”œâ”€â”€ public/                # Static assets
â”‚   â””â”€â”€ package.json           # React dependencies
â”‚
â”œâ”€â”€ Data Scraping/             # Python data processing pipeline
â”‚   â”œâ”€â”€ datasetscript.py       # Web scraping script
â”‚   â”œâ”€â”€ cleaneddata.py         # Data cleaning utilities
â”‚   â””â”€â”€ output.txt             # Scraped data output
â”‚
â””â”€â”€ README.md                  # This documentation
```

## ğŸ› ï¸ Installation & Setup

### Prerequisites

- **Node.js** (v18 or higher)
- **Python** (v3.8 or higher)
- **Ollama** (for local LLM)
- **Pinecone Account** (for vector database)

### Step 1: Clone the Repository

```bash
git clone <repository-url>
cd LiveLikeALocal
```

### Step 2: Backend Setup

```bash
cd Backend

# Install Node.js dependencies
npm install

# Create environment file
cp .env.example .env
```

Configure your `.env` file with the following variables:

```env
# Ollama Configuration
OLLAMA_HOST=http://localhost:11434
OLLAMA_CHAT_MODEL=llama3.2

# Pinecone Configuration
PINECONE_API_KEY=your_pinecone_api_key
PINECONE_INDEX_NAME=your_index_name

# Embedding Model
EMBEDDING_MODEL=Xenova/all-MiniLM-L6-v2

# Server Configuration
PORT=8000
```

### Step 3: Frontend Setup

```bash
cd Frontend

# Install React dependencies
npm install
```

### Step 4: Data Pipeline Setup

```bash
cd "Data Scraping"

# Install Python dependencies
pip install requests beautifulsoup4 langchain openai pinecone-client sentence-transformers
```

### Step 5: Start Ollama

Ensure Ollama is running with the required model:

```bash
# Install and start Ollama (if not already installed)
ollama serve

# Pull the required model
ollama pull llama3.2
```

## ğŸš€ Running the Application

### Development Mode

1. **Start the Backend Server**:
   ```bash
   cd Backend
   npm start
   ```
   The server will run on `http://localhost:8000`

2. **Start the Frontend Development Server**:
   ```bash
   cd Frontend
   npm start
   ```
   The React app will run on `http://localhost:3000`

3. **Run Data Pipeline** (if needed):
   ```bash
   cd "Data Scraping"
   python datasetscript.py
   ```

### Production Deployment

1. **Build the Frontend**:
   ```bash
   cd Frontend
   npm run build
   ```

2. **Deploy Backend**:
   ```bash
   cd Backend
   npm start
   ```

## ğŸ”§ Configuration

### System Prompt Customization

The system behavior is controlled by the prompt in `Backend/Initialprompt.md`. This file contains the core instructions for the AI assistant, including:

- Domain translation guidelines
- Communication protocols
- Knowledge bridging strategies
- Project-specific context

### Model Configuration

You can customize the LLM and embedding models by modifying the environment variables:

- `OLLAMA_CHAT_MODEL`: The chat model to use (default: llama3.2)
- `EMBEDDING_MODEL`: The embedding model for semantic search (default: Xenova/all-MiniLM-L6-v2)

### Vector Database Setup

1. Create a Pinecone account at [pinecone.io](https://pinecone.io)
2. Create a new index with appropriate dimensions for your embedding model
3. Configure the index name and API key in your `.env` file

## ğŸ“Š API Endpoints

### Chat Management

- `POST /api/chat/start` - Start a new chat session
- `GET /api/chat/:chatId/history` - Retrieve chat history
- `DELETE /api/chat/:chatId` - Clear chat history

### Message Processing

- `POST /api/chat` - Process a message with context and history
- `POST /api/local-chat` - Process a message without chat history

### Response Format

```json
{
  "response": "AI-generated response",
  "chatHistory": [
    {
      "role": "user",
      "content": "User message"
    },
    {
      "role": "assistant", 
      "content": "AI response"
    }
  ]
}
```

## ğŸ§ª Testing

The project includes comprehensive test files:

- `test-chat.js` - Chat functionality testing
- `test-ollama.js` - Ollama integration testing
- `test-embeddings.js` - Embedding generation testing
- `test-endpoint.js` - API endpoint testing

Run tests with:
```bash
cd Backend
node test-chat.js
```

## ğŸ” Usage Examples

### Starting a New Chat

1. Open the application in your browser
2. Click "New Chat" in the sidebar
3. Begin typing your message
4. The system will provide context-aware responses

### Interdisciplinary Questions

**Example 1 - CS to Biology Translation:**
```
User: "How does a machine learning algorithm work for DNA sequence analysis?"
Assistant: "Think of machine learning like teaching a computer to recognize patterns in DNA sequences, similar to how biologists identify genetic markers..."
```

**Example 2 - Biology to CS Translation:**
```
User: "What are the computational challenges of DNA sequencing errors?"
Assistant: "DNA sequencing errors create data quality issues that require robust error correction algorithms, similar to how software handles corrupted data..."
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“ Development Notes

### Key Implementation Details

1. **Embedding Generation**: Uses Transformers.js for client-side embedding generation
2. **Vector Search**: Implements semantic search using Pinecone vector database
3. **Chat Persistence**: In-memory chat history management with session IDs
4. **Error Handling**: Comprehensive error handling and user feedback
5. **Performance**: Optimized for real-time conversation with minimal latency

### Future Enhancements

- [ ] Database persistence for chat history
- [ ] User authentication and profiles
- [ ] Advanced document processing pipeline
- [ ] Multi-language support
- [ ] Integration with external knowledge bases
- [ ] Real-time collaboration features

## ğŸ“„ License

This project is licensed under the ISC License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- **Ollama** for providing local LLM capabilities
- **Pinecone** for vector database infrastructure
- **Hugging Face** for embedding models and transformers
- **React** and **Node.js** communities for excellent documentation

## ğŸ“ Support

For questions, issues, or contributions, please:

1. Check the existing issues in the repository
2. Create a new issue with detailed information
3. Contact the development team

---

**LiveLikeALocal** - Bridging the gap between Computer Science and Biology through intelligent conversation. ğŸ§¬ğŸ’» 